{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63f76c8-2de1-4970-bb71-521b132be409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "    dataset = preprocessing_dataset(dataset, label_type)\n",
    "    return dataset\n",
    "\n",
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset\n",
    "\n",
    "dataset_path = r\"/opt/ml/input/data/train/train.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "\n",
    "train, vali = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_train.txt\", sep='\\t', index=False)\n",
    "vali[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_vali.txt\", sep='\\t', index=False)\n",
    "\n",
    "dataset_train = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_train.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "dataset_vali = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_vali.txt\", field_indices=[0,1], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e167ca2f-7049-492c-a38c-006fac1a2ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e6885-e66a-446b-9e0d-98245af5c2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10da0c7b-2e66-45cd-b7e0-3f07286eb6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['히틀러 [SEP] 독일 [SEP] 무하의 애국심은 히틀러의 독일 정부가 그를의 적으로 간주한 명부에 포함할 만큼 유명했다.', '7'],\n",
       " ['촉군 [SEP] 위나라 [SEP] 위나라가 퇴각하면서 일대의 주민과 가축을 몽땅 데려가 버렸기 때문에 무도 음평에 주둔한 촉군은 인력과 식량의 부족에 시달렸고 제갈량은 다시 한중으로 돌아갔다.',\n",
       "  '0'],\n",
       " ['삼성카드 [SEP] 삼성그룹 [SEP] 이는 삼성그룹 회장인 이건희가 아들인 이재용에게 경영권을 인계하여 에버랜드-삼성생명-삼성카드-삼성전자- 에버랜드로 이어지는 순환출자에 의한 그룹 지배를 확보하는데 있어 중요한 역할을 하였다.',\n",
       "  '3'],\n",
       " [\"대전방송 [SEP] TJB [SEP] 청원인은 'TJB 대전방송'에서 6년을 근무하며 그중 3년 6개월을 '8뉴스' 앵커로 일하고 퇴사했지만, 퇴직금을 받지 못했다고 주장했다.\",\n",
       "  '6'],\n",
       " ['찰스 라이엘 [SEP] 지질학 [SEP] 저명한 교수로는 물리학자 에드워드 빅터 애플턴, 오언 윌런스 리처드슨, 물리학자/수학자 제임스 맥스웰, 생물물리학자 로절린드 프랭클린, 생물학자 줄리언 헉슬리, 지질학자 찰스 라이엘, 외과의사 조지프 리스터, 역사가 아널드 J. 토인비 등이 있다.',\n",
       "  '2']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c53a8f-6a9a-4ef6-812f-b019ca28c14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정교회 [SEP] 대주교 [SEP] 콥트 정교회 교황 요셉 2세가 최초의 에티오피아인 출신 대주교로 임명한 아부나 바실리오스는 에티오피아 테와히도 정교회의 초대 총대주교가 되었다.',\n",
       " '10']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vali[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f08a1c-30fc-4da7-9d84-2d982b57dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c058940-971b-4095-a4ff-f5070f1b863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8def715-edae-4d4c-bb9b-aa0216da6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.01\n",
    "num_epochs = 6\n",
    "max_grad_norm = 1\n",
    "log_interval = 50\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30504347-581e-4148-8f44-0ec9665acb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_vali = BERTDataset(dataset_vali, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "vali_dataloader = torch.utils.data.DataLoader(data_vali, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dbb394c-0696-4090-9451-0c6eb90b1d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3a2335b-ebc5-43c8-acdd-3c0cb51c7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 42,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "#         print(pooler)\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "# model = BERTClassifier(bertmodel).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd6a7df8-cfe5-4946-a412-1f9569cd8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = LabelSmoothingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b813ef72-77df-42d5-ab67-206efae764a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f734f84-7494-4fc9-8705-00f9af688b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68552f47-b2b9-485b-ae20-6ea58540516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 3.903625965118408 train acc 0.0\n",
      "epoch 1 batch id 51 loss 1.7728376388549805 train acc 0.4338235294117647\n",
      "epoch 1 batch id 101 loss 1.2574117183685303 train acc 0.557240099009901\n",
      "epoch 1 batch id 151 loss 1.1993439197540283 train acc 0.6072019867549668\n",
      "epoch 1 batch id 201 loss 0.7840970754623413 train acc 0.6340174129353234\n",
      "epoch 1 train acc 0.6443055555555556\n",
      "epoch 1 test acc 0.6677631578947368\n",
      "epoch 2 batch id 1 loss 1.3913025856018066 train acc 0.6875\n",
      "epoch 2 batch id 51 loss 0.8508379459381104 train acc 0.7199754901960784\n",
      "epoch 2 batch id 101 loss 0.8175843954086304 train acc 0.744430693069307\n",
      "epoch 2 batch id 151 loss 0.8247249126434326 train acc 0.7555877483443708\n",
      "epoch 2 batch id 201 loss 0.5799598097801208 train acc 0.7644589552238806\n",
      "epoch 2 train acc 0.7681944444444444\n",
      "epoch 2 test acc 0.6864035087719298\n",
      "epoch 3 batch id 1 loss 1.1091325283050537 train acc 0.6875\n",
      "epoch 3 batch id 51 loss 0.9209443926811218 train acc 0.8088235294117647\n",
      "epoch 3 batch id 101 loss 0.8297867774963379 train acc 0.8236386138613861\n",
      "epoch 3 batch id 151 loss 0.5938364267349243 train acc 0.828228476821192\n",
      "epoch 3 batch id 201 loss 0.2094932198524475 train acc 0.832089552238806\n",
      "epoch 3 train acc 0.8329166666666666\n",
      "epoch 3 test acc 0.7012061403508771\n",
      "epoch 4 batch id 1 loss 0.8758524060249329 train acc 0.6875\n",
      "epoch 4 batch id 51 loss 0.672984778881073 train acc 0.8455882352941176\n",
      "epoch 4 batch id 101 loss 0.5410966277122498 train acc 0.8579826732673267\n",
      "epoch 4 batch id 151 loss 0.501044750213623 train acc 0.8613410596026491\n",
      "epoch 4 batch id 201 loss 0.3196064233779907 train acc 0.8656716417910447\n",
      "epoch 4 train acc 0.8694444444444445\n",
      "epoch 4 test acc 0.6995614035087719\n",
      "epoch 5 batch id 1 loss 0.7248203754425049 train acc 0.75\n",
      "epoch 5 batch id 51 loss 0.42784440517425537 train acc 0.8890931372549019\n",
      "epoch 5 batch id 101 loss 0.4509735703468323 train acc 0.8994430693069307\n",
      "epoch 5 batch id 151 loss 0.37877240777015686 train acc 0.9002483443708609\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "#         print(type(token_ids.long().to(device)))\n",
    "#         print(type(valid_length))\n",
    "#         print(type(segment_ids.long().to(device)))\n",
    "#         print(type(label.long().to(device)))\n",
    "#         break\n",
    "#     break\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(vali_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"/opt/ml/model/model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e9635-d7d4-477b-8c5f-525f4acf80d0",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add47e3-40fa-4fe0-8d9e-461723a8c06d",
   "metadata": {},
   "source": [
    "## test dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c1542-1343-4c00-8e92-e1c13aede8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"/opt/ml/input/data/test/test.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "\n",
    "dataset[['sentence','label']].to_csv(\"/opt/ml/input/data/test/test.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842200f-b37c-4934-a06f-d555f88d1a05",
   "metadata": {},
   "source": [
    "## test dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f23a66bc-a0c7-43b7-9f5b-e2c6dc8a7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset(\"/opt/ml/input/data/test/test.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1bbd5-8ecf-4d21-b146-062dbef90054",
   "metadata": {},
   "source": [
    "## infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec015d37-f4bb-4632-8a39-2349b33d524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/ml/model/model_state_dict.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "Predict = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    _, predict = torch.max(out,1)\n",
    "    Predict.extend(predict.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275378d-c696-49eb-8021-f3d5d92de30c",
   "metadata": {},
   "source": [
    "## Reslut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fc9e1e1-968f-4675-85f0-ea9abe2be262",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(Predict, columns=['pred'])\n",
    "output.to_csv('/opt/ml/result/submission_kobert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd576cd-702c-4485-a208-191d2cd3625a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
